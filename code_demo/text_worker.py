"""
Text Generation Worker (Consumer) - Processes tasks from RabbitMQ queue
"""
import json
import time
import random
from typing import Dict, Any
import pika
import redis
from settings import settings


class TextGenerationWorker:
    """Worker class that consumes tasks and processes them"""
    
    def __init__(self):
        """Initialize connections to RabbitMQ and Redis"""
        self.rabbitmq_connection = None
        self.rabbitmq_channel = None
        self.redis_client = None
        self._connect()
    
    def _connect(self):
        """Establish connections to RabbitMQ and Redis"""
        try:
            # Connect to RabbitMQ
            credentials = pika.PlainCredentials(
                settings.RABBITMQ_USER,
                settings.RABBITMQ_PASSWORD
            )
            parameters = pika.ConnectionParameters(
                host=settings.RABBITMQ_HOST,
                port=settings.RABBITMQ_PORT,
                credentials=credentials,
                virtual_host=settings.RABBITMQ_VHOST
            )
            self.rabbitmq_connection = pika.BlockingConnection(parameters)
            self.rabbitmq_channel = self.rabbitmq_connection.channel()
            
            # Declare queue (idempotent)
            self.rabbitmq_channel.queue_declare(
                queue=settings.TEXT_GENERATION_QUEUE,
                durable=settings.QUEUE_DURABLE
            )
            
            # Set QoS - process one message at a time
            self.rabbitmq_channel.basic_qos(prefetch_count=settings.PREFETCH_COUNT)
            
            # Connect to Redis
            self.redis_client = redis.Redis(
                host=settings.REDIS_HOST,
                port=settings.REDIS_PORT,
                password=settings.REDIS_PASSWORD,
                db=settings.REDIS_DB,
                decode_responses=settings.REDIS_DECODE_RESPONSES
            )
            
            # Test Redis connection
            self.redis_client.ping()
            
            print("‚úì Worker connected to RabbitMQ and Redis")
            
        except Exception as e:
            print(f"‚úó Connection error: {e}")
            raise
    
    def generate_text_mock(self, prompt: str) -> str:
        """
        Mock text generation function - simulates LLM latency
        
        ** DESIGN NOTE: Replace this function with actual LLM API call **
        
        Example OpenAI integration:
        ```python
        import openai
        openai.api_key = settings.OPENAI_API_KEY
        
        response = openai.ChatCompletion.create(
            model=settings.OPENAI_MODEL,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=settings.OPENAI_MAX_TOKENS,
            temperature=settings.OPENAI_TEMPERATURE
        )
        return response.choices[0].message.content
        ```
        
        Args:
            prompt: Text prompt to generate from
            
        Returns:
            Generated text (mocked)
        """
        # Simulate variable processing time (2-5 seconds)
        processing_time = random.uniform(
            settings.MOCK_MIN_DELAY,
            settings.MOCK_MAX_DELAY
        )
        time.sleep(processing_time)
        
        # Generate mock response based on prompt keywords
        mock_responses = {
            "story": "Once upon a time, in a world of circuits and code, there lived a curious robot named Atlas. "
                    "Atlas discovered the beauty of colors and began painting digital masterpieces that inspired both "
                    "humans and machines alike. The end.",
            
            "quantum": "Quantum computing leverages quantum mechanics principles like superposition and entanglement "
                      "to perform calculations. Unlike classical bits (0 or 1), qubits can exist in multiple states "
                      "simultaneously, enabling parallel processing of vast computational problems.",
            
            "python": "def validate_email(email: str) -> bool:\n"
                     "    import re\n"
                     "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n"
                     "    return bool(re.match(pattern, email))",
            
            "haiku": "Silicon minds dream,\n"
                    "Learning patterns, finding truth,\n"
                    "Future unfolds now.",
            
            "default": f"Generated response for: '{prompt[:50]}...'\n\n"
                      "This is a mock response. In production, this would be generated by an LLM such as "
                      "GPT-4, Claude, or other language models. The actual implementation would call the "
                      "respective API and return the generated text."
        }
        
        # Find matching response or use default
        prompt_lower = prompt.lower()
        for keyword, response in mock_responses.items():
            if keyword in prompt_lower:
                return response
        
        return mock_responses["default"]
    
    def update_task_status(self, task_id: str, status: str, result: str = None, error: str = None):
        """
        Update task status in Redis
        
        Args:
            task_id: Task identifier
            status: New status (PROCESSING, COMPLETED, FAILED)
            result: Generated text result (if completed)
            error: Error message (if failed)
        """
        try:
            redis_key = settings.get_task_key(task_id)
            task_data = self.redis_client.get(redis_key)
            
            if not task_data:
                print(f"‚ö†Ô∏è  Task {task_id[:8]}... not found in Redis")
                return
            
            # Update task data
            task_dict = json.loads(task_data)
            task_dict["status"] = status
            task_dict["updated_at"] = time.time()
            
            if result:
                task_dict["result"] = result
                task_dict["completed_at"] = time.time()
            
            if error:
                task_dict["error"] = error
                task_dict["failed_at"] = time.time()
            
            # Save back to Redis with TTL
            self.redis_client.setex(
                redis_key,
                settings.TASK_TTL,
                json.dumps(task_dict)
            )
            
        except Exception as e:
            print(f"‚úó Error updating task status: {e}")
    
    def process_task(self, task_data: Dict[str, Any]) -> bool:
        """
        Process a single task
        
        Args:
            task_data: Task payload from RabbitMQ
            
        Returns:
            True if successful, False otherwise
        """
        task_id = task_data.get("task_id", "unknown")
        prompt = task_data.get("prompt", "")
        
        try:
            print(f"\n‚öôÔ∏è  Processing task: {task_id[:8]}...")
            print(f"   Prompt: {prompt[:60]}...")
            
            # Update status to PROCESSING
            self.update_task_status(task_id, settings.STATUS_PROCESSING)
            
            # Generate text (this is where you'd call OpenAI API)
            start_time = time.time()
            generated_text = self.generate_text_mock(prompt)
            elapsed_time = time.time() - start_time
            
            print(f"   ‚úì Generation complete ({elapsed_time:.2f}s)")
            print(f"   Result preview: {generated_text[:80]}...")
            
            # Update status to COMPLETED with result
            self.update_task_status(
                task_id,
                settings.STATUS_COMPLETED,
                result=generated_text
            )
            
            print(f"   üíæ Saved to Redis: {settings.get_task_key(task_id)}")
            return True
            
        except Exception as e:
            print(f"   ‚úó Error processing task: {e}")
            
            # Update status to FAILED with error
            self.update_task_status(
                task_id,
                settings.STATUS_FAILED,
                error=str(e)
            )
            return False
    
    def callback(self, ch, method, properties, body):
        """
        Callback function triggered when message is received
        
        Args:
            ch: Channel
            method: Delivery method
            properties: Message properties
            body: Message body (JSON)
        """
        try:
            # Parse task data
            task_data = json.loads(body)
            
            # Process the task
            success = self.process_task(task_data)
            
            # Acknowledge message
            if success:
                ch.basic_ack(delivery_tag=method.delivery_tag)
            else:
                # Reject and don't requeue to avoid infinite loops
                ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)
                
        except json.JSONDecodeError as e:
            print(f"‚úó Invalid JSON in message: {e}")
            ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)
            
        except Exception as e:
            print(f"‚úó Unexpected error in callback: {e}")
            ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)
    
    def start(self):
        """Start consuming messages from the queue"""
        try:
            print("\n" + "="*70)
            print("ü§ñ Text Generation Worker Started")
            print("="*70)
            print(f"Queue: {settings.TEXT_GENERATION_QUEUE}")
            print(f"Prefetch: {settings.PREFETCH_COUNT}")
            print("\n‚è≥ Waiting for tasks... (Press Ctrl+C to stop)")
            print("="*70)
            
            # Start consuming
            self.rabbitmq_channel.basic_consume(
                queue=settings.TEXT_GENERATION_QUEUE,
                on_message_callback=self.callback
            )
            
            self.rabbitmq_channel.start_consuming()
            
        except KeyboardInterrupt:
            print("\n\n‚úì Worker stopped by user")
            self.stop()
            
        except Exception as e:
            print(f"\n‚úó Worker error: {e}")
            self.stop()
    
    def stop(self):
        """Stop the worker and close connections"""
        try:
            if self.rabbitmq_channel and self.rabbitmq_channel.is_open:
                self.rabbitmq_channel.stop_consuming()
            
            if self.rabbitmq_connection and not self.rabbitmq_connection.is_closed:
                self.rabbitmq_connection.close()
            
            print("‚úì Worker connections closed")
            
        except Exception as e:
            print(f"‚úó Error closing connections: {e}")


def main():
    """Main entry point for the worker"""
    worker = TextGenerationWorker()
    worker.start()


if __name__ == "__main__":
    main()
